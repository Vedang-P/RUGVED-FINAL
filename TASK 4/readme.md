# ğŸ§  Task 4: Artificial Neural Network from Scratch on MNIST

This task involves implementing an Artificial Neural Network (ANN) **completely from scratch**, without using high-level libraries like TensorFlow or PyTorch. You will train the network on the **MNIST handwritten digit dataset** using **gradient descent** and implement all the key components manually.

---

## ğŸ“Œ Objective

- Build an ANN from scratch
- Understand and implement:
  - Forward propagation
  - Backpropagation
  - Loss functions (e.g., Cross-Entropy)
  - Gradient Descent Optimizer
  - Activation functions (ReLU, Sigmoid, etc.)
- Train on the **MNIST dataset**
- Evaluate performance on unseen test data

---

## ğŸ“š Resources

- [3Blue1Brown â€“ Deep Learning, Visualized](https://youtu.be/aircAruvnKk?si=E7aowG9bgxeQ-CoI)

Highly recommended before beginning â€” this video explains the "why" behind each concept youâ€™ll implement.

---

## ğŸ—‚ï¸ Files Included

| File Name                 | Description                                      |
|--------------------------|--------------------------------------------------|
| `ann_from_scratch.py`    | Python script implementing ANN manually          |
| `mnist_loader.py`        | Utility to download/load MNIST data (optional)  |
| `README.md`              | This file                                       |
| `report.pdf` (optional)  | Brief summary of approach, accuracy, and insights|

---

## ğŸ“Š Dataset: MNIST

The **MNIST** dataset contains 70,000 grayscale images (28x28) of handwritten digits (0â€“9).

You can download it using:
```python
from sklearn.datasets import fetch_openml
X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
